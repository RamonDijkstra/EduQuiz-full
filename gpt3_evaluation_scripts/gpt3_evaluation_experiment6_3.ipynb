{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ced4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8a7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables to perform evalution for you task\n",
    "quiz_path = os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/experiment_6/generated_quiz.json'\n",
    "answer_path = os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/experiment_6/generated_answers_2.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b66793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get quizzes\n",
    "quizzes = []\n",
    "with open(quiz_path) as f:\n",
    "    generated_quizzes = json.load(f)\n",
    "\n",
    "for key in generated_quizzes:\n",
    "    quizzes.append(generated_quizzes[key])\n",
    "\n",
    "# Get questions\n",
    "answers = []\n",
    "with open(answer_path) as f:\n",
    "    generated_answers = json.load(f)\n",
    "\n",
    "for key in generated_answers:\n",
    "    answers.append(generated_answers[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9dfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the predictions and gold references in lists and calculate exact match and f1 score\n",
    "\n",
    "predictions = []\n",
    "gold_references = []\n",
    "predictions_list = []\n",
    "gold_references_list = []\n",
    "\n",
    "for i in range(len(quizzes)):\n",
    "    \n",
    "    clean_completion = quizzes[i].split('True answer: ')[1].split(\"\\nFalse answer\")[0]\n",
    "    \n",
    "    generated_completion = answers[i]\n",
    "    clean_generated_completion = generated_completion.split('Answer: ')[1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    predictions.append(clean_generated_completion)\n",
    "    gold_references.append(clean_completion)\n",
    "\n",
    "    predictions_list.append(clean_generated_completion.split(' '))\n",
    "    gold_references_list.append([clean_completion.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7514ed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = datasets.load_metric('bleu')\n",
    "rouge = datasets.load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957df6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu.add_batch(predictions=predictions_list, references=gold_references_list)\n",
    "rouge.add_batch(predictions=predictions, references=gold_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc14283",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bleu = bleu.compute()\n",
    "final_rouge = rouge.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21078e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of downloaded meteor from https://www.cs.cmu.edu/~alavie/METEOR/\n",
    "meteor_path = \"<PATH_TO_YOUR_DOWNLOADED_METEOR\"\n",
    "\n",
    "# Move results to meteor directory\n",
    "with open(meteor_path + \"predictions.txt\", 'w') as f:\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(predictions[i] + '\\n')\n",
    "        \n",
    "with open(meteor_path + \"ground_truth.txt\", 'w') as f:\n",
    "    for i in range(len(gold_references)):\n",
    "        f.write(gold_references[i] + '\\n')\n",
    "\n",
    "# Run the meteor command from the meteor directory and remove result files again   \n",
    "wd = os.getcwd()\n",
    "os.chdir(meteor_path)\n",
    "output = os.popen(\"java -Xmx2G -jar meteor-*.jar predictions.txt ground_truth.txt -l en -norm\").read()\n",
    "os.remove(meteor_path + \"predictions.txt\")\n",
    "os.remove(meteor_path + \"ground_truth.txt\")\n",
    "os.chdir(wd)\n",
    "\n",
    "# Get the score from the output\n",
    "meteor_score = round(float(output.split(\"Final score:\")[1].strip()) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "402a53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result files\n",
    "\n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/experiment_6/' + 'generated_answers_2.txt', 'w') as f:\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(predictions[i] + '\\n')\n",
    "        \n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/experiment_6/' + 'generated_answers.txt', 'w') as f:\n",
    "    for i in range(len(gold_references)):\n",
    "        f.write(gold_references[i] + '\\n')\n",
    "        \n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_evaluation_scripts')[0] + 'generated_data_gpt3/experiment_6/' + 'automatic_evaluation_answers.txt', 'w') as f:\n",
    "    f.write(\"BLEU: \" + str(round(final_bleu['bleu'] * 100, 2)))\n",
    "    f.write('\\n')\n",
    "    f.write(\"ROUGE-L: \" + str(round(final_rouge['rougeL'].mid.fmeasure * 100, 2)))\n",
    "    f.write('\\n')\n",
    "    f.write(\"METEOR: \" + str(meteor_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

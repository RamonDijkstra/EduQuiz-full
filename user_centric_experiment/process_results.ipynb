{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f280f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:228: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41aa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latex setting \n",
    "matplotlib.use(\"pgf\")\n",
    "matplotlib.rcParams.update({\n",
    "    \"pgf.texsystem\": \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "    'figure.autolayout' : True,\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 10,\n",
    "    'figure.max_open_warning': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13442fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load case study results\n",
    "df = pd.read_csv(\"raw_results.csv\")\n",
    "\n",
    "# Load original and generated quizzes\n",
    "original_quiz_path = os.path.abspath(os.getcwd()).split('user_centric_experiment')[0] + 'processed_data/gpt3/completion_6/processed_test.json'\n",
    "generated_quiz_path = os.path.abspath(os.getcwd()).split('user_centric_experiment')[0] + 'generated_data_gpt3/experiment_6/generated_quiz.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee31ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prompts\n",
    "prompts = []\n",
    "\n",
    "for line in open(original_quiz_path):\n",
    "    prompts.append(json.loads(line)['prompt'].split('\\n\\n###')[0].strip())\n",
    "\n",
    "# Get original quizzes\n",
    "original_quizzes = []\n",
    "\n",
    "for line in open(original_quiz_path):\n",
    "    original_quizzes.append(json.loads(line)['completion'].split('\\n###')[0].strip())\n",
    "    \n",
    "# Get generated quizzes\n",
    "generated_quizzes = []\n",
    "with open(generated_quiz_path) as f:\n",
    "    generated_quizzes_dict = json.load(f)\n",
    "\n",
    "for key in generated_quizzes_dict:\n",
    "    generated_quizzes.append(generated_quizzes_dict[key].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c507d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ID's for which we tested the user on\n",
    "sampled_idx = [0, 54, 63, 126, 135, 144, 153, 171, 279, 351, 414, 450, 531, 639, 648, 756, 774, 819, 828, 882]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c09bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order everything and clean up\n",
    "labels = df.keys().tolist()\n",
    "questions = df.iloc[0].tolist()\n",
    "opinion_questions = df.iloc[0].tolist()[-15:]\n",
    "opinion_questions_labels = labels[-15:]\n",
    "\n",
    "opinion_questions_dict = dict(zip(opinion_questions_labels, opinion_questions))\n",
    "\n",
    "df = df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e8ea7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original quiz answers\n",
    "orq_answers = [\"Because her family was very poor.\",\n",
    "              \"He and his friends were helping his neighbor moving a couch.\",\n",
    "              \"Byron Bay and Sydney\",\n",
    "              \"somewhat interact with the students\",\n",
    "              \"A language learning environment.\",\n",
    "              \"150\",\n",
    "              \"The Australian government tries to make immigrants feel at home.\",\n",
    "              \"Rates of death from illnesses have risen due to global warming.\",\n",
    "              \"They had to wait a long time and play catch-up when checking in.\",\n",
    "              \"Spending much time on Facebook affected her study\",\n",
    "              \"China allows climbers of any age to climb Mt. Everest.\",\n",
    "              \"Going to see a doctor.\",\n",
    "              \"Planets can receive heat generated deep inside the planet.\",\n",
    "              \"The rock becomes unclear.\",\n",
    "              \"A book.\",\n",
    "              \"Milk\",\n",
    "              \"Maths.\",\n",
    "              \"A bike.\",\n",
    "              \"He helped the traffic police at a crossroad.\",\n",
    "              \"The film You Are the Apple of My Eye.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90929e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated quiz answers\n",
    "geq_answers = [\"Sorry.\",\n",
    "              \"She disappeared from the spot where she was playing.\",\n",
    "              \"You can have a beer in Germany or drink a hot dog in America.\",\n",
    "              \"The future is uncertain.\",\n",
    "              \"Connecting with native speakers.\",\n",
    "              \"The early-warning system.\",\n",
    "              \"Its beautiful beaches.\",\n",
    "              \"He has serious doubts about it.\",\n",
    "              \"It took a long time to open.\",\n",
    "              \"About 83% of them.\",\n",
    "              \"To wear a piece of rock from the top of the world.\",\n",
    "              \"Poor nutrition.\",\n",
    "              \"Water will exist underground as long as the planet is not too hot or too cold.\",\n",
    "              \"Air movements in the earth's atmosphere.\",\n",
    "              \"Because it is a way to relax themselves.\",\n",
    "              \"Tea.\",\n",
    "              \"Because he thought maths was interesting.\",\n",
    "              \"August 5th.\",\n",
    "              \"He helped others.\",\n",
    "              \"He took drugs with some friends.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe3943",
   "metadata": {},
   "source": [
    "# Section 8.1 - Grade Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10b36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the grade for each participant\n",
    "orq_grades = []\n",
    "geq_grades = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    given_orq_answers = row[[\"ORQ1\",\"ORQ2\",\"ORQ3\",\"ORQ4\",\"ORQ5\",\"ORQ6\",\"ORQ7\",\"ORQ8\",\"ORQ9\",\"ORQ10\",\"ORQ11\",\"ORQ12\",\"ORQ13\",\"ORQ14\",\"ORQ15\",\"ORQ16\",\"ORQ17\",\"ORQ18\",\"ORQ19\",\"ORQ20\"]].tolist()\n",
    "    given_geq_anwers = row[[\"GEQ1\",\"GEQ2\",\"GEQ3\",\"GEQ4\",\"GEQ5\",\"GEQ6\",\"GEQ7\",\"GEQ8\",\"GEQ9\",\"GEQ10\",\"GEQ11\",\"GEQ12\",\"GEQ13\",\"GEQ14\",\"GEQ15\",\"GEQ16\",\"GEQ17\",\"GEQ18\",\"GEQ19\",\"GEQ20\"]].tolist()\n",
    "    \n",
    "    orq_grade = round(len(set(given_orq_answers) & set(orq_answers)) / len(given_orq_answers) * 10)\n",
    "    geq_grade = round(len(set(given_geq_anwers) & set(geq_answers)) / len(given_geq_anwers) * 10)\n",
    "    \n",
    "    orq_grades.append(orq_grade)\n",
    "    geq_grades.append(geq_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62a60828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade Distribution Original quizzes\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0, 24])\n",
    "plt.yticks(np.arange(0, 24+1, 6))\n",
    "\n",
    "values, _, _ = plt.hist(orq_grades, rwidth=.8, bins=np.arange(0, 10+2) - 0.5)\n",
    "\n",
    "plt.xticks(np.arange(0, 10+1, 1.0))\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "rects = ax.patches\n",
    "values = [int(x) for x in values]\n",
    "  \n",
    "for rect, label in zip(rects, values):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height+0.25, label,\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(w=3.2, h=2.2)    \n",
    "\n",
    "ax.set_title('(a) Original Quizzes', y=0, pad=-35, verticalalignment=\"top\")\n",
    "\n",
    "plt.savefig('figures/grade_distribution_original_quizzes.pgf')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a43528a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade Distribution Generated quizzes\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0, 24])\n",
    "plt.yticks(np.arange(0, 24+1, 6))\n",
    "\n",
    "values, _, _ = plt.hist(geq_grades, rwidth=.8, bins=np.arange(0, 10+2) - 0.5)\n",
    "\n",
    "plt.xticks(np.arange(0, 10+1, 1.0))\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Make some labels.\n",
    "rects = ax.patches\n",
    "values = [int(x) for x in values]\n",
    "  \n",
    "for rect, label in zip(rects, values):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width() / 2, height+0.25, label,\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(w=3.2, h=2.2)    \n",
    "\n",
    "ax.set_title('(b) EduQuiz-Generated Quizzes', y=0, pad=-35, verticalalignment=\"top\")\n",
    "\n",
    "plt.savefig('figures/grade_distribution_generated_quizzes.pgf')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a534a",
   "metadata": {},
   "source": [
    "# Section 8.2 - Unanswerable Quizzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70639dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure\n",
    "# x-axis questions\n",
    "# y-axis amount of student who got it right\n",
    "\n",
    "orx_axis = [\"ORQ1\",\"ORQ2\",\"ORQ3\",\"ORQ4\",\"ORQ5\",\"ORQ6\",\"ORQ7\",\"ORQ8\",\"ORQ9\",\"ORQ10\",\"ORQ11\",\"ORQ12\",\"ORQ13\",\"ORQ14\",\"ORQ15\",\"ORQ16\",\"ORQ17\",\"ORQ18\",\"ORQ19\",\"ORQ20\"]\n",
    "ory_axis = []\n",
    "ory_axis_unans = []\n",
    "ory_axis_false = []\n",
    "\n",
    "gex_axis = [\"GEQ1\",\"GEQ2\",\"GEQ3\",\"GEQ4\",\"GEQ5\",\"GEQ6\",\"GEQ7\",\"GEQ8\",\"GEQ9\",\"GEQ10\",\"GEQ11\",\"GEQ12\",\"GEQ13\",\"GEQ14\",\"GEQ15\",\"GEQ16\",\"GEQ17\",\"GEQ18\",\"GEQ19\",\"GEQ20\"]\n",
    "gey_axis = []\n",
    "gey_axis_unans = []\n",
    "gey_axis_false = []\n",
    "\n",
    "for i in range(len(orq_answers)):\n",
    "    count_right_answers = df[orx_axis[i]].value_counts()[orq_answers[i]]\n",
    "    \n",
    "    try:\n",
    "        count_unanswerable = df[orx_axis[i]].value_counts()[\"This question is unanswerable.\"]\n",
    "    except:\n",
    "        count_unanswerable = 0\n",
    "        \n",
    "    count_false = len(df) - count_right_answers - count_unanswerable\n",
    "    \n",
    "    ory_axis.append(count_right_answers)\n",
    "    ory_axis_unans.append(count_unanswerable)\n",
    "    ory_axis_false.append(count_false)\n",
    "\n",
    "for i in range(len(geq_answers)):\n",
    "    try:\n",
    "        count_right_answers = df[gex_axis[i]].value_counts()[geq_answers[i]]\n",
    "    except:\n",
    "        count_right_answers = 0\n",
    "        \n",
    "    try:\n",
    "        count_unanswerable = df[gex_axis[i]].value_counts()[\"This question is unanswerable.\"]\n",
    "    except:\n",
    "        count_unanswerable = 0\n",
    "        \n",
    "    count_false = len(geq_answers) - count_right_answers - count_unanswerable\n",
    "    \n",
    "    gey_axis.append(count_right_answers)\n",
    "    gey_axis_unans.append(count_unanswerable)\n",
    "    gey_axis_false.append(count_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ab03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Quizzes that are unanswerable\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.yticks(np.arange(0, 24+1, 6))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.xlabel(\"Question ID\")\n",
    "plt.ylabel(\"Count\")\n",
    "unans = ax.bar(orx_axis, ory_axis_unans, width=0.8)\n",
    "\n",
    "ax.set_xticks(np.arange(len(orx_axis)), orx_axis)\n",
    "ax.set_yticks(np.arange(0, 24+1, 6))\n",
    "ax.set_ylim([0, 24])\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "ax.bar_label(unans, padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(w=6.0, h=2.2)    \n",
    "\n",
    "ax.set_title('(a) Original Quizzes', y=0, pad=-55, verticalalignment=\"top\")\n",
    "\n",
    "plt.savefig('figures/unanswerable_original_quizzes.pgf')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4033a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated Quizzes that are unanswerable\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.yticks(np.arange(0, 24+1, 6))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.xlabel(\"Question ID\")\n",
    "plt.ylabel(\"Count\")\n",
    "unans = ax.bar(gex_axis, gey_axis_unans, width=0.8)\n",
    "\n",
    "ax.set_xticks(np.arange(len(gex_axis)), gex_axis)\n",
    "ax.set_yticks(np.arange(0, 24+1, 6))\n",
    "ax.set_ylim([0, 24])\n",
    "\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "\n",
    "ax.bar_label(unans, padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(w=6.0, h=2.2)    \n",
    "\n",
    "ax.set_title('(b) EduQuiz-generated Quizzes', y=0, pad=-55, verticalalignment=\"top\")\n",
    "\n",
    "plt.savefig('figures/unanswerable_generated.pgf')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a0a04",
   "metadata": {},
   "source": [
    "# Section 8.3 - Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ea6086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate grades without rounding\n",
    "orq_grades = []\n",
    "geq_grades = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    given_orq_answers = row[[\"ORQ1\",\"ORQ2\",\"ORQ3\",\"ORQ4\",\"ORQ5\",\"ORQ6\",\"ORQ7\",\"ORQ8\",\"ORQ9\",\"ORQ10\",\"ORQ11\",\"ORQ12\",\"ORQ13\",\"ORQ14\",\"ORQ15\",\"ORQ16\",\"ORQ17\",\"ORQ18\",\"ORQ19\",\"ORQ20\"]].tolist()\n",
    "    given_geq_anwers = row[[\"GEQ1\",\"GEQ2\",\"GEQ3\",\"GEQ4\",\"GEQ5\",\"GEQ6\",\"GEQ7\",\"GEQ8\",\"GEQ9\",\"GEQ10\",\"GEQ11\",\"GEQ12\",\"GEQ13\",\"GEQ14\",\"GEQ15\",\"GEQ16\",\"GEQ17\",\"GEQ18\",\"GEQ19\",\"GEQ20\"]].tolist()\n",
    "    \n",
    "    orq_grade = len(set(given_orq_answers) & set(orq_answers)) / len(given_orq_answers) * 10\n",
    "    geq_grade = len(set(given_geq_anwers) & set(geq_answers)) / len(given_geq_anwers) * 10\n",
    "    \n",
    "    orq_grades.append(orq_grade)\n",
    "    geq_grades.append(geq_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab13f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Generated quiz answers\n",
    "filtered_geq_answers = [\"She disappeared from the spot where she was playing.\",\n",
    "              \"The future is uncertain.\",\n",
    "              \"Connecting with native speakers.\",\n",
    "              \"The early-warning system.\",\n",
    "              \"He has serious doubts about it.\",\n",
    "              \"To wear a piece of rock from the top of the world.\",\n",
    "              \"Air movements in the earth's atmosphere.\",\n",
    "              \"Because it is a way to relax themselves.\",\n",
    "              \"Tea.\",\n",
    "              \"August 5th.\",\n",
    "              \"He helped others.\",\n",
    "              \"He took drugs with some friends.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac6f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate filtered grades without rounding\n",
    "    \n",
    "filtered_geq_grades = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    given_geq_anwers = row[[\"GEQ2\",\"GEQ4\",\"GEQ5\",\"GEQ6\",\"GEQ8\",\"GEQ11\",\"GEQ14\",\"GEQ15\",\"GEQ16\",\"GEQ18\",\"GEQ19\",\"GEQ20\"]].tolist()\n",
    "    geq_grade = len(set(given_geq_anwers) & set(filtered_geq_answers)) / len(given_geq_anwers) * 10\n",
    "    filtered_geq_grades.append(geq_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3394313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfiltered Scatterplot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.xticks(np.arange(0, 10+1, 1.0))\n",
    "plt.yticks(np.arange(0, 10+1, 1.0))\n",
    "\n",
    "plt.xlabel('Original Quizzes Grade')\n",
    "plt.ylabel('Generated Quizzes Grade')\n",
    "\n",
    "p_corr, p_value = pearsonr(orq_grades, geq_grades)\n",
    "\n",
    "b, m = polyfit(orq_grades, geq_grades, 1)\n",
    "\n",
    "ax.set_xlim([0, 10])\n",
    "ax.set_ylim([0, 10])\n",
    "\n",
    "x = np.arange(0, 10+1, 1.0)\n",
    "\n",
    "plt.scatter(orq_grades, geq_grades)\n",
    "plt.plot(x, b + m * x, '-')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(w=3.2, h=3.2)  \n",
    "\n",
    "ax.set_title(f\"(a) Unfiltered\\n Pearson's correlation (r): {p_corr:.3f} \\np-value: {p_value:.3f}\", y=0, pad=-35, verticalalignment=\"top\")\n",
    "\n",
    "plt.savefig('figures/scatterplot_unfiltered.pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ac2962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Scatterplot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.xticks(np.arange(0, 10+1, 1.0))\n",
    "plt.yticks(np.arange(0, 10+1, 1.0))\n",
    "\n",
    "plt.xlabel('Original Quizzes Grade')\n",
    "plt.ylabel('Filtered Generated Quizzes Grade')\n",
    "\n",
    "p_corr, p_value = pearsonr(orq_grades, filtered_geq_grades)\n",
    "\n",
    "b, m = polyfit(orq_grades, filtered_geq_grades, 1)\n",
    "\n",
    "ax.set_xlim([0, 10])\n",
    "ax.set_ylim([0, 10])\n",
    "\n",
    "x = np.arange(0, 10+1, 1.0)\n",
    "\n",
    "plt.scatter(orq_grades, filtered_geq_grades)\n",
    "plt.plot(x, b + m * x, '-')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.set_size_inches(w=3.2, h=3.2)  \n",
    "\n",
    "ax.set_title(f\"(b) Filtered\\n Pearson's correlation (r): {p_corr:.3f} \\np-value: {p_value:.3f}\", y=0, pad=-35, verticalalignment=\"top\")\n",
    "\n",
    "plt.savefig('figures/scatterplot_filtered.pgf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e1a6e",
   "metadata": {},
   "source": [
    "# Section 9.1 & 9.2 - General and Detailed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f19aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quiz_results(sampled_id, quiz_name):\n",
    "    x_axis = df[quiz_name].value_counts().index.tolist()        \n",
    "    y_axis = df[quiz_name].value_counts().values.tolist()        \n",
    "    \n",
    "    labels = x_axis\n",
    "\n",
    "    x = np.arange(len(x_axis))  # the label locations\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    rects1 = ax.bar(x_axis, y_axis, width)\n",
    "\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Guessed answers')\n",
    "    ax.set_xticks(x, labels)\n",
    "    ax.set_yticks(np.arange(0, 24+1, 6))\n",
    "    ax.set_ylim([0, 24])\n",
    "    plt.yticks(np.arange(0, 24+1, 6))\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    ax.bar_label(rects1, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.savefig(f'raw_quiz_result_plots/{quiz_name}.png')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7381453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_labels = [\"ORQ1\",\"ORQ2\",\"ORQ3\",\"ORQ4\",\"ORQ5\",\"ORQ6\",\"ORQ7\",\"ORQ8\",\"ORQ9\",\"ORQ10\",\"ORQ11\",\"ORQ12\",\"ORQ13\",\"ORQ14\",\"ORQ15\",\"ORQ16\",\"ORQ17\",\"ORQ18\",\"ORQ19\",\"ORQ20\"]\n",
    "\n",
    "for i in range(len(sampled_idx)):\n",
    "    print_quiz_results(sampled_idx[i], original_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c9f2bad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_labels = [\"GEQ1\",\"GEQ2\",\"GEQ3\",\"GEQ4\",\"GEQ5\",\"GEQ6\",\"GEQ7\",\"GEQ8\",\"GEQ9\",\"GEQ10\",\"GEQ11\",\"GEQ12\",\"GEQ13\",\"GEQ14\",\"GEQ15\",\"GEQ16\",\"GEQ17\",\"GEQ18\",\"GEQ19\",\"GEQ20\"]\n",
    "\n",
    "for i in range(len(sampled_idx)):\n",
    "    print_quiz_results(sampled_idx[i], generated_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6a002",
   "metadata": {},
   "source": [
    "# Generated Quiz 1 - unanswerable\n",
    "19 times flagged as unanswerable\n",
    "\n",
    "# Generated Quiz 2 - switch true/false answer\n",
    "The answer that is most chosen (21) times is true but not flagged as the true answer. The flagged true answer is true but not the only thing that is true\n",
    "\n",
    "# Generated Quiz 3 - unanswerable\n",
    "6 times flagged as unanswerable\n",
    "\n",
    "# Generated Quiz 4 - good quiz (slight modification)\n",
    "The answers given are quite divided. The distractors can be true\n",
    "\n",
    "# Generated Quiz 5 - good quiz\n",
    "18 people got them right. 6 chose a closely related distractor\n",
    "\n",
    "# Generated Quiz 6 - switch true/false answer\n",
    "Hard quiz. True/false answer should be switched\n",
    "\n",
    "# Generated Quiz 7 - unanswerable - distractors are true\n",
    "Distractors can be true.\n",
    "\n",
    "# Generated Quiz 8 - good quiz (slight modification)\n",
    "Viable answers. Distractor could be true\n",
    "\n",
    "# Generated Quiz 9 - unanswerable - distractors are true\n",
    "Unanswerable 10 times\n",
    "\n",
    "# Generated Quiz 10 - unanswerable\n",
    "One distractor have to be changed\n",
    "\n",
    "# Generated Quiz 11 - switch true/false answer\n",
    "True/false answer should be switched\n",
    "\n",
    "# Generated Quiz 12 - unanswerable - distractors are true\n",
    "Unanswerable 12 times\n",
    "\n",
    "# Generated Quiz 13 - unanswerable\n",
    "10 times unanswerable. Double distractors\n",
    "\n",
    "# Generated Quiz 14 - good quiz\n",
    "19 got right. 5 got wrong\n",
    "\n",
    "# Generated Quiz 15 - good quiz\n",
    "20 got right. 4 wrong\n",
    "\n",
    "# Generated Quiz 16 - good quiz\n",
    "19 got right\n",
    "\n",
    "# Generated Quiz 17 - unnaswerable - distractors are true\n",
    "16 times unanswerable\n",
    "\n",
    "# Generated Quiz 18 - good quiz (slight modification)\n",
    "24 times good\n",
    "\n",
    "# Generated Quiz 19 - switch true/false answer\n",
    "good quiz. switch true/false answer\n",
    "\n",
    "# Generated Quiz 20 - good quiz\n",
    "21 times good.\n",
    "\n",
    "# Summary\n",
    "- Good Quiz:                       5\n",
    "- Good Quiz Slight Modification:   3\n",
    "- Good Quiz but switch true/false: 4\n",
    "- Unanswerable Distractors true:   4\n",
    "- Unanswerable:                    4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ecc24",
   "metadata": {},
   "source": [
    "# Section 10.1 & 10.2 - Quiz Quality and AI in Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "089a1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_opinion_resultsh(sampled_id, quiz_name, opinion_options):\n",
    "    y_axis = opinion_options\n",
    "    x_axis = []\n",
    "    \n",
    "    for opinion in opinion_options:\n",
    "        count = df[quiz_name].to_list().count(opinion)\n",
    "        x_axis.append(count)\n",
    "        \n",
    "    labels = y_axis\n",
    "    \n",
    "    if quiz_name == \"OPQ5\":\n",
    "        labels = [\"Never\", \"Someti\", \"AboHal\", \"Most\", \"Always\"]\n",
    "    \n",
    "    if quiz_name == 'OPQ6':\n",
    "        labels = [\"Disrup\", \"Hindra\", \"Neutra\", \"NotBot\", \"DidnNo\"]\n",
    "        \n",
    "    if quiz_name in ['OPQ9', 'OPQ10', 'OPQ11', 'OPQ12']:\n",
    "        labels = [\"StrDis\", \"SomDis\", \"NaNd\", \"SomAgr\", \"StrAgr\"]\n",
    "\n",
    "    y = np.arange(len(labels))\n",
    "    width = 0.5\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,6))\n",
    "    rects1 = ax.barh(y_axis, x_axis, width)\n",
    "\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_yticks(y, labels)\n",
    "    ax.set_xticks(np.arange(0, 24+1, 6))\n",
    "    ax.set_xlim([0, 24])\n",
    "    plt.xticks(np.arange(0, 24+1, 6))\n",
    "\n",
    "    ax.bar_label(rects1, padding=3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.set_size_inches(w=3.2, h=2.5)  \n",
    "    \n",
    "    ax.set_title(test_questions[quiz_name], y=0, pad=-40, verticalalignment=\"top\", wrap=True)\n",
    "    plt.savefig(f'figures/{quiz_name}.pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1b6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_options = [\n",
    "                   [\"Never\", \"Sometimes\", \"About half the time\", \"Most of the time\", \"Always\"],\n",
    "                   [\"Disruptive / I got very confused\", \"Hindrance / It slowed me down\", \"Neutral / I just picked the best answer\", \"Not bothered / Also human tests have imperfect quizzes\", \"Didn't notice / I only focused on answering the quizzes\"],\n",
    "                   [\"Strongly disagree\", \"Somewhat disagree\", \"Neither agree nor disagree\", \"Somewhat agree\", \"Strongly agree\"],\n",
    "                   [\"Strongly disagree\", \"Somewhat disagree\", \"Neither agree nor disagree\", \"Somewhat agree\", \"Strongly agree\"],\n",
    "                   [\"Strongly disagree\", \"Somewhat disagree\", \"Neither agree nor disagree\", \"Somewhat agree\", \"Strongly agree\"],\n",
    "                   [\"Strongly disagree\", \"Somewhat disagree\", \"Neither agree nor disagree\", \"Somewhat agree\", \"Strongly agree\"]\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00d7704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_questions_labels = ['OPQ5', 'OPQ6', 'OPQ9', 'OPQ10', 'OPQ11', 'OPQ12']\n",
    "\n",
    "test_questions = {\"OPQ5\": \"(a) How often did you recognize\\nthat there was an AI-generated\\nquiz during the test?\\n\",\\\n",
    "\"OPQ6\": \"(b) One or more of the AI-\\ngenerated quizzes may be\\nimperfect. How did this affect\\nyour overall test experience?\",\\\n",
    "\"OPQ9\": \"(a) AI can have a positive impact\\non the educational domain.\\n\\n\",\\\n",
    "\"OPQ10\": \"(b) I would use Automatic Quiz\\nGeneration to test my knowledge\\nwhile learning.\\n\",\\\n",
    "\"OPQ11\": \"(c) Automatic Quiz Generation\\ncould help teachers to simplify\\nthe process of creating tests.\\n\",\\\n",
    "\"OPQ12\": \"(d) Research on AI-powered\\neducation is important.\\n\\n\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d094fa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, key in enumerate(opinion_questions_labels):\n",
    "    print_opinion_resultsh(opinion_questions_dict[key], key, opinion_options=opinion_options[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb62983",
   "metadata": {},
   "source": [
    "# Section 10.3 - Open questions (in Latex template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4a71244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name three strengths of the AI generated quizzes \n",
      "\n",
      "P1 & The questions were mostly specific, often the answer was just in the text, and the questions were often gramatically correct \\\\\n",
      "P2 & Not superficial... \\\\\n",
      "P3 & Releative clear stories compared to the human generated ones. \\\\\n",
      "P4 & Easy, quick and low cost to make \\\\\n",
      "P5 & Cheap, no human intervention, scalable \\\\\n",
      "P6 & It’s easier to make, it’s faster and a lot of quizzes can be made in a short time period \\\\\n",
      "P7 & Fact based, efficiënt, high level questions \\\\\n",
      "P8 & Convenient, interesting, \\\\\n",
      "P9 & understandable, clear, good \\\\\n",
      "P10 & - \\\\\n",
      "P11 & Grammar, structure & level \\\\\n",
      "P12 & Specific questions, answer not that hard to find, clear paragraphs \\\\\n",
      "P13 & Fast, reliable and creative \\\\\n",
      "P14 & Often realistic, similar too human quizes, double anwsers \\\\\n",
      "P15 & Ease, uniqueness, bias \\\\\n",
      "P16 & No spelling mistakes, Logical answering options, No clear wrong answers (similarity in answers is good) \\\\\n",
      "P17 & Less time needed for human interference, AI can be self-learning and adaptability  \\\\\n",
      "P18 & Fast, diverse, cheap \\\\\n",
      "P19 & Made sense, were relevant and were simple \\\\\n",
      "P20 & / \\\\\n",
      "P21 & - \\\\\n",
      "P22 & It's quicker, efficient and easily accessible for many \\\\\n",
      "P23 & nan \\\\\n",
      "P24 & Testing whether one read the text, less (human) time to make, ...  \\\\\n"
     ]
    }
   ],
   "source": [
    "print(opinion_questions_dict[\"OPQ7\"], '\\n')\n",
    "for i in range(len(df[\"OPQ7\"])):\n",
    "    print(f'P{i+1} &', df[\"OPQ7\"].iloc[i], '\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62fc2c8b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name three limitations of the AI generated quizzes \n",
      "\n",
      "P1 & Some questions were vague, some questions were identical, some questions had the same answer multiple times \\\\\n",
      "P2 & Not always answerable, too vague,  \\\\\n",
      "P3 & The answer often contained the same words as the text, to easy to search. \\\\\n",
      "P4 & High chance of questions that dont make sense, the AI does not take into account what is the key lesson that the student should be tested on, it just replicates the tests it was trained on. Teachers have better insight in what they want the test to test. \\\\\n",
      "P5 & Double answers, incorrect answers, no explanation \\\\\n",
      "P6 & The quizzes can have some errors, the AI might misunderstand the texts and when a lot of quizzes are generated it is hard to check them \\\\\n",
      "P7 & Limitation of interpretation, difficulty emotion based questions, need to review at beginning of learning cycle \\\\\n",
      "P8 & Not able to read context, no proper language, same questions asked \\\\\n",
      "P9 & long and much \\\\\n",
      "P10 & - \\\\\n",
      "P11 & Answers looked similar, answers did not always look righ. \\\\\n",
      "P12 & Not always representative answer options,  \\\\\n",
      "P13 & Not understandable, incorrect with text and too easy \\\\\n",
      "P14 & Strange wording, unclear, interpretation sometimes used \\\\\n",
      "P15 & Not thought over, sometimes does not make sense. \\\\\n",
      "P16 & All answers not really answering the question, Questions focusing on one line of text, Weird questions (e.g. about writer's feelings)  \\\\\n",
      "P17 & Expensive, human error is able to hinder the process of AI and AI isn't fully capable of understanding the human learning process yet! \\\\\n",
      "P18 & Incorrect question/answer, style, no personal feedback \\\\\n",
      "P19 & Questions sometimes not with straightforward answers \\\\\n",
      "P20 & / \\\\\n",
      "P21 & - \\\\\n",
      "P22 & Sentence structure can be disrupted sometimes, less emotional texts, and sometimes the texts can seem like a sumup \\\\\n",
      "P23 & nan \\\\\n",
      "P24 & Repeating answers, unanswerable questions, often answers pretty straightforward.  \\\\\n"
     ]
    }
   ],
   "source": [
    "print(opinion_questions_dict[\"OPQ8\"], '\\n')\n",
    "for i in range(len(df[\"OPQ8\"])):\n",
    "    print(f'P{i+1} &', df[\"OPQ8\"].iloc[i], '\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "096d4aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List three reasons why we would support research into AI-powered education. \n",
      "\n",
      "P1 & It takes away a lot of time for teachers, it can also help students by generating endless quizes to practice, and can unlock all the texts on the internet \\\\\n",
      "P2 & Possbility to bring higher level education to more people easily,... \\\\\n",
      "P3 & nan \\\\\n",
      "P4 & There are various risks involved, these should be studied thoroughly. It might reduce the work teachers need to do. It might help students prepare for tests  \\\\\n",
      "P5 & Makes it easier to provide education in less populated regions, makes it easier to create more educational content, might get better than humans at some point \\\\\n",
      "P6 & It can help improve the overall education and it can help teachers giving beter education \\\\\n",
      "P7 & Efficiency, accuracy, from reseach point of view \\\\\n",
      "P8 & nan \\\\\n",
      "P9 & fast, improvement, future \\\\\n",
      "P10 & - \\\\\n",
      "P11 &   Studying opportunities, easy to crrate exams for teachers and maybe to be used to create online learning for third world countries \\\\\n",
      "P12 & It takes work of the hands of teachers, hence they could give more attention to students \\\\\n",
      "P13 & Teacher shortage, more efficient and extra home schooling device \\\\\n",
      "P14 & Time saving for teachers, money saving, modern way of education \\\\\n",
      "P15 & It could be helpful for memorization, it could help gain insight from other perspectives, more time on more personal education \\\\\n",
      "P16 & It's performance will only improve and since education is one of the most important pillars of society, it should be one of most important areas to apply AI in (together with healthcare). \\\\\n",
      "P17 & Less time needed for human interference, AI can be adaptable and it could be an investment in the future \\\\\n",
      "P18 & Faster generated, cheaper, broader audience \\\\\n",
      "P19 & Teachers can spend more time focusing on teaching rather than creating tests. Students can test themselves more frequently. Knowledge can be measured more easily. \\\\\n",
      "P20 & / \\\\\n",
      "P21 & - \\\\\n",
      "P22 & It's cost efficient, less pressure on teachers, easily accessible for many people \\\\\n",
      "P23 & nan \\\\\n",
      "P24 & Focussing more on educating and less on making tests; good for checking if someone read the text; could be very useful in grading tests as well, again shifting the focus of educators on teaching rather than testing.    \\\\\n"
     ]
    }
   ],
   "source": [
    "print(opinion_questions_dict[\"OPQ13\"], '\\n')\n",
    "for i in range(len(df[\"OPQ13\"])):\n",
    "    print(f'P{i+1} &', df[\"OPQ13\"].iloc[i], '\\\\\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07441f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you see any ethical or societal risks, or other barriers to successful deployment of AI in education? \n",
      "\n",
      "P1 & Using all texts on the internet might also come with risks so safeguards need to be in place to prevent harmful language. I think as we currently stand, there still needs to be a human to check the texts and answers to make sure that it is all correct and questions are answerable \\\\\n",
      "P2 & Bias in data, even more than human bias of teachers currently \\\\\n",
      "P3 & nan \\\\\n",
      "P4 & Yes, it can be hard to make sure that an AI tests the right knowledge. It is also hard to determine what tests are good and which are not (in terms of what data to train on), prevent racism or other things that might occur in tests created by humans. \\\\\n",
      "P5 & Possibly no/incomplete explanation of the answer, bias in training data leads to bias in created content \\\\\n",
      "P6 & When the AI malfunctions it can have ethical or societal risks \\\\\n",
      "P7 & No \\\\\n",
      "P8 & nan \\\\\n",
      "P9 & No, but I would always have the teacher check it. \\\\\n",
      "P10 & - \\\\\n",
      "P11 & Scared boomers that are afraid the AI will take over the world \\\\\n",
      "P12 & No \\\\\n",
      "P13 & No filter, so mean or racist consequences could happen \\\\\n",
      "P14 & Human relationship with teacher is missed. Parents will not like that. \\\\\n",
      "P15 & Maybe \\\\\n",
      "P16 & As long as all data is anonymized (very important!), I do not see any risks or barriers to succesful deployment of AI in education. \\\\\n",
      "P17 & Certain subjects that are more sensitive to certain humans could be hard to be recognized by AI \\\\\n",
      "P18 & Yes. Everyone could have a different opinion about the correctness of such an AI powered education algorithm  \\\\\n",
      "P19 & Non very significant \\\\\n",
      "P20 & / \\\\\n",
      "P21 & - \\\\\n",
      "P22 & If there is a mistake in the text, someone cannot be deemed responsible. Additionally, the quality of some texts were poor so there should be more incentives to better the quality of AI-text making \\\\\n",
      "P23 & nan \\\\\n",
      "P24 & No, as long as questions are well constructed and answers matched with high confidence \\\\\n"
     ]
    }
   ],
   "source": [
    "print(opinion_questions_dict[\"OPQ14\"], '\\n')\n",
    "for i in range(len(df[\"OPQ14\"])):\n",
    "    print(f'P{i+1} &', df[\"OPQ14\"].iloc[i], '\\\\\\\\')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

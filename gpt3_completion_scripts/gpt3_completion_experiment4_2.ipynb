{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ced4de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d8a7f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these variables to perform evalution for you task\n",
    "test_path = os.path.abspath(os.getcwd()).split('gpt3_completion_scripts')[0] + 'processed_data/gpt3/completion_4/processed_test.json'\n",
    "generated_path = os.path.abspath(os.getcwd()).split('gpt3_completion_scripts')[0] + 'generated_data_gpt3/experiment_4/generated_distractors.json'\n",
    "input_macaw_path = os.path.abspath(os.getcwd()).split('gpt3_completion_scripts')[0] + 'generated_data_gpt3/experiment_4/input_macaw.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b66793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the target prompts and completions\n",
    "test_data = []\n",
    "\n",
    "for line in open(test_path):\n",
    "    test_data.append((json.loads(line)))\n",
    "    \n",
    "generated_data = []\n",
    "# Get the generated completions\n",
    "with open(generated_path) as f:\n",
    "    generated = json.load(f)\n",
    "\n",
    "for key in generated:\n",
    "    generated_data.append(generated[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d9dfcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the predictions and gold references in lists and calculate exact match and f1 score\n",
    "\n",
    "predictions = []\n",
    "gold_references = []\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    \n",
    "    completion = test_data[i]['completion']\n",
    "    clean_completion = completion.split('False answer: ')\n",
    "    distractor_1 = clean_completion[1].split('\\n')[0]\n",
    "    distractor_2 = clean_completion[2].split('\\n')[0]\n",
    "    distractor_3 = clean_completion[3].split('\\n')[0]\n",
    "    \n",
    "    gold_references.append([distractor_1, distractor_2, distractor_3])\n",
    "    \n",
    "    generated_completion = generated_data[i]\n",
    "    clean_generated_completion = generated_completion.split('False answer:')\n",
    "    generated_distractor_1 = clean_generated_completion[1].split('\\n')[0].strip()\n",
    "    generated_distractor_2 = clean_generated_completion[2].split('\\n')[0].strip()\n",
    "    generated_distractor_3 = clean_generated_completion[3].split('\\n')[0].strip()\n",
    "\n",
    "    predictions.append([generated_distractor_1, generated_distractor_2, generated_distractor_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "402a53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create result files\n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_completion_scripts')[0] + 'generated_data_gpt3/experiment_4/' + 'predictions_distractors.txt', 'w') as f:\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(predictions[i][0] + '\\n')\n",
    "        f.write(predictions[i][1] + '\\n')\n",
    "        f.write(predictions[i][2] + '\\n\\n')\n",
    "        \n",
    "with open(os.path.abspath(os.getcwd()).split('gpt3_completion_scripts')[0] + 'generated_data_gpt3/experiment_4/' + 'ground_truth_distractors.txt', 'w') as f:\n",
    "    for i in range(len(gold_references)):\n",
    "        f.write(gold_references[i][0] + '\\n')\n",
    "        f.write(gold_references[i][1] + '\\n')\n",
    "        f.write(gold_references[i][2] + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f037b99",
   "metadata": {},
   "source": [
    "# Create input file for testing accuracy with Macaw-11b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400071c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "macaw_data_list = []\n",
    "    \n",
    "for i in range(len(test_data)):\n",
    "    temp = test_data[i]['prompt']\n",
    "    text = temp.split('\\n\\nQuestion: ')[0]\n",
    "    question = temp.split('\\n\\nQuestion: ')[1].split('\\n\\nAnswer:')[0]\n",
    "    answer = temp.split('\\n\\nAnswer: ')[1].split('\\n\\n###')[0]\n",
    "    option_a = answer\n",
    "    option_b = predictions[i][0]\n",
    "    option_c = predictions[i][1]\n",
    "    option_d = predictions[i][2]\n",
    "    \n",
    "    macaw_prompt_text = \"$answer$\" + \" ; \" + \"$question$ = \" + question + \" $mcoptions$ = (A) \" + option_a + \\\n",
    "                        \" (B) \" + option_b + \" (C) \" + option_c + \" (D) \" + option_d + \" $context$ = \" + text    \n",
    "    macaw_completion = answer\n",
    "    \n",
    "    macaw_dict = {\"prompt\": macaw_prompt_text, \"completion\": macaw_completion}\n",
    "    macaw_data_list.append(macaw_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2694780e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_macaw_path, 'w') as f:\n",
    "    for item in macaw_data_list:\n",
    "        f.write(json.dumps(item) + \"\\n\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
